# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_OlWFn0egHpYGkZTWPyDQBcnMys1hOw7
"""



from google.colab import drive
drive.mount('/content/drive/')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt 
import seaborn as sns
from sklearn.cluster import KMeans 
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import cv2 as cv
import os
from os import listdir
from os.path import isfile, join
from PIL import Image
import random
# %matplotlib inline

"""# Q1)

# Preprocessing
"""

col=['class','Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']
data=pd.read_csv('/content/drive/MyDrive/wine.data',names=col)

data.head()

data=data.dropna()

feat=['Alcohol','Malic acid','Ash','Alcalinity of ash','Magnesium','Total phenols','Flavanoids','Nonflavanoid phenols','Proanthocyanins','Color intensity','Hue','OD280/OD315 of diluted wines','Proline']
x_data=data.drop(columns='class').values
# print(x_data)
x_data=StandardScaler().fit_transform(x_data)
# x_data=pd.DataFrame(data = x_data, columns = feat)

"""# Dimension reduction and Data visualization"""

pca = PCA(n_components=2)

x_data_pca = pca.fit_transform(x_data)

x_data_pca = pd.DataFrame(data = x_data_pca, columns = ['principal component 1', 'principal component 2'])

x_data_pca.head()

y_data=data[['class']]

data=pd.concat([x_data_pca,y_data],axis=1)
data.head()

np.unique(y_data)

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1,1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 Component PCA', fontsize = 20)


targets = [1, 2, 3]
colors = ['r', 'g', 'b']
for target, color in zip(targets,colors):
    indicesToKeep = data['class'] == target
    ax.scatter(data.loc[indicesToKeep, 'principal component 1'], data.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)
ax.legend(targets)
ax.grid()

"""# K-means clustering algorithm"""

kmeans = KMeans(n_clusters = 3)
y_kmeans = kmeans.fit_predict(x_data)

plt.figure(figsize=(10,5))
plt.scatter(x_data[y_kmeans == 0, 0], x_data[y_kmeans == 0, 1], s = 100, c = 'red', label = '1')
plt.scatter(x_data[y_kmeans == 1, 0], x_data[y_kmeans == 1, 1], s = 100, c = 'orange', label = '2')
plt.scatter(x_data[y_kmeans == 2, 0], x_data[y_kmeans == 2, 1], s = 100, c = 'green', label = '3')

#Plotting the centroids of the cluster
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:,1], s = 100, c = 'black', label = 'Centroids')

plt.legend()
plt.show()

"""# Checking for multiple value of no of cluster"""

from sklearn.metrics import silhouette_score

score=[]

for i in range(2,11):
    kmeans = KMeans(n_clusters = i)
    y_kmeans_loop=kmeans.fit_predict(x_data)
    scr= silhouette_score(x_data,y_kmeans_loop , sample_size=None, random_state=0)
    score.append(scr)

print(score)

plt.plot(range(2,11),score,color = 'red')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')
plt.show()



"""# Elbow Method"""

score_wcss=[]
for i in range(1,11):
    kmeans_2 = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans_2.fit(x_data)
    # y_kmeans_loop_2=kmeans.fit(x_data)
    # wcss= silhouette_score(x_data,y_kmeans_loop , sample_size=None, random_state=0)
    # kmeans = KMeans(n_clusters = i)
    wcss=kmeans_2.inertia_
    score_wcss.append(wcss)

plt.plot(range(1,11),score_wcss,color = 'blue')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.show()

"""# Q2

# A) and B) making k means from scratch
"""



train_q2 = pd.read_csv('/content/drive/MyDrive/file lab 9/fashion-mnist_train.csv')
test_q2 = pd.read_csv('/content/drive/MyDrive/file lab 9/fashion-mnist_test.csv')

x_training_q2=train_q2.drop(columns='label').values
# x_training_q2=StandardScaler.fit_transform(x_training_q2)
# x_training_q2

x_testing_q2=train_q2.drop(columns='label').values
# x_testing_q2=StandardScaler.fit_transform(x_testing_q2)
# x_testing_q2

y_testing_q2=train_q2[['label']].values
# y_testing_q2

y_training_q2=train_q2[['label']].values
# y_training_q2

m = x_training_q2.shape[0]
n = x_training_q2.shape[1]
n
n_iter=10
k = 10
# n is no of features
# m is no of rows

def decision_tree_making(data,original_training_data,feature_space,target,depth):

    x=np.unique(data[target],return_counts=True)
    class_with_max_frequency=x[0][np.argmax(x[1])]
    if(len(feature_space)==0 or (depth_check(depth)==0)):
        # if number of feature or attribute left is equal to 
        return(class_with_max_frequency)
    elif(len(data)==0):
        # if there is no data for this branch than return class with max frequency in original training data
        return(np.argmax(np.unique(original_training_data[target],return_counts=True)[1]))
    elif(len(x[0])<=1):
        # if number of classes in data equal to 1
        return(x[0][0])
    else:
        best_split_attribute, max_info= find_best_attribute(data,feature_space,target)
        value=np.unique(data[best_split_attribute])
        tree={best_split_attribute:{} }
        sub_feature_space=[]
        for i in feature_space:
            if(i!=best_split_attribute):
                sub_feature_space.append(i)
        depth+=1
        if(check_info_gain(max_info)==0):
            return(class_with_max_frequency)
        else:
            for i in value:
              sub_data=split_data_on_feature(data,best_split_attribute,i)
              subtree= decision_tree_making(sub_data, original_training_data,sub_feature_space,target,depth)
              tree[best_split_attribute][i]=subtree
    return(tree)
col=train1.columns
tree1 = decision_tree_making(train1,train1,col[1:],col[0],0)
print(tree1)

"""# C training k means"""

Op, Centres = kmeans(n,m,n_iter,k, x_training_q2)

for i in range(1,11):
  print(Op[i].shape[0])

clusters = Centres.T

"""# D Plotting 10 centroids

# E Plotting first 10 images of each cluster

#2(f)
"""

def decision_tree_making(data,original_training_data,feature_space,target,depth):

    x=np.unique(data[target],return_counts=True)
    class_with_max_frequency=x[0][np.argmax(x[1])]
    if(len(feature_space)==0 or (depth_check(depth)==0)):
        # if number of feature or attribute left is equal to 
        return(class_with_max_frequency)
    elif(len(data)==0):
        # if there is no data for this branch than return class with max frequency in original training data
        return(np.argmax(np.unique(original_training_data[target],return_counts=True)[1]))
    elif(len(x[0])<=1):
        # if number of classes in data equal to 1
        return(x[0][0])
    else:
        best_split_attribute, max_info= find_best_attribute(data,feature_space,target)
        value=np.unique(data[best_split_attribute])
        tree={best_split_attribute:{} }
        sub_feature_space=[]
        for i in feature_space:
            if(i!=best_split_attribute):
                sub_feature_space.append(i)
        depth+=1
        if(check_info_gain(max_info)==0):
            return(class_with_max_frequency)
        else:
            for i in value:
              sub_data=split_data_on_feature(data,best_split_attribute,i)
              subtree= decision_tree_making(sub_data, original_training_data,sub_feature_space,target,depth)
              tree[best_split_attribute][i]=subtree
    return(tree)
col=train1.columns
tree1 = decision_tree_making(train1,train1,col[1:],col[0],0)
print(tree1)

for i in range(1,11):
  print(Op_2[i].shape[0])

"""#Visualising cluster centers"""

clustersb = Centresb.T

"""# G visulazing images from each cluster"""

for i in range(10):
  a = Op_2[i+1]
  a = a[0:10]
  print('New class: ')
  for j in range(10):
    b = a[j]
    plt.imshow(b.reshape(28,28), interpolation='nearest')
    plt.show()

"""# H Calculating SSE"""

def calculate_sse(op, center_s, k):

  sum=0

  for i in range(k):
    sum+=np.sum((op[1+i] - center_s[i])**2)
  
  return sum

sse_a = calculate_sse(Op, clusters, k)
sse_b = calculate_sse(Op_2, clustersb, k)
print(sse_a)
print(sse_b)

"""SSE comes less for part f

# Q3

# Importing and Normalizing Data
"""

yes_address='/content/drive/MyDrive/file lab 9/yes/'
no_address='/content/drive/MyDrive/file lab 9/no/'
list_yes = sorted(os.listdir(yes_address))        
list_no = sorted(os.listdir(no_address))

list_yes_address=[]
for i in list_yes:
  list_yes_address.append(yes_address+str(i))

list_no_address=[]
for i in list_no:
  list_no_address.append(no_address+str(i))
df_yes=pd.DataFrame()
df_yes['file']=list_yes_address
df_yes['class']=[1 for i in range (0,len(list_yes))]
df_no=pd.DataFrame()
df_no['file']=list_no_address
df_no['class']=[0 for i in range (0,len(list_no))]
df_address=pd.concat([df_yes,df_no])

df_address['file'].shape

li=[]

for i in df_address['file']:    
    img_ar = Image.open(i).convert('L')
    img_ar=img_ar.resize((28,28))
    li.append(list(img_ar.getdata()))

dataset_q3=pd.DataFrame(li)
dataset_q3

dataset_q3=StandardScaler().fit_transform(dataset_q3)

"""# Dimension Reduction"""

pca_q3 = PCA(n_components=400)
pca_dataset_q3=pca_q3.fit_transform(dataset_q3)



print('No of points in Yes community are: ', len(list_yes))
print('No of points in No community are: ', len(list_no))

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
li = []

for i in range(1, 11):
    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
    kmeans.fit(dataset_q3)
    li.append(kmeans.inertia_)

plt.plot(range(1, 11), li)
plt.title('The elbow method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS') # Within cluster sum of squares
plt.show()

"""# Visualizing the communities"""

plt.imshow(dataset_q3[0].reshape(28,28))
plt.show()

"""# Agglomerative hierarchical clustering"""

import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(pca_dataset_q3, method='ward'))
plt.axhline(y=700, color='b', linestyle='--')
plt.show()

from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  
cluster_pred = cluster.fit_predict(pca_dataset_q3)
print(cluster_pred)

"""# K-means"""

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)
y_kmeans = kmeans.fit_predict(pca_dataset_q3)

print(len(y_kmeans[y_kmeans==0]))
print(len(cluster_pred[cluster_pred==0]))